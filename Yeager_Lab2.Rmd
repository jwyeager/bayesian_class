---
---
---

Lab 2 - Components of Bayes' Theorem

Joel Yeager

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
```

**Problem**

You will estimate the posterior distribution for the mean number of individuals of an invasive plant species per m2 in a disturbed grassland. We will call that mean θ. You have prior information telling you that the average number of these plants per m2 is 10.2 with a standard deviation of the mean = .5. You have a set of fifty observations in hand obtained by sweaty labor in the field. Execute the following steps.

1.  Simulate 50 data points from a Poisson distribution with mean θ = 6.4 to represent the data set. What is the variance? Be sure to put the R function set.seed(3) before the call to rpois() to assure that we all get the same results. Call the data vector y.

```{r}
set.seed(3)

y <- rpois(50, 6.4)
```

2.  Set values for the prior mean (mu.prior) and standard deviation (sigma.prior).

```{r}
mu.prior <- 10.2
sigma.prior <- 0.5
```

3.  Set up a vector containing a sequence of values for θ, the mean number of invasive plants. You want this vector to approximate a continuous θ so be sure it contains values that are not too far apart. Use code like this: theta = seq(0,15,step) where you set step = .01. Setting a value for step with global scope is important. You will use it later when you integrate.

```{r}
theta <- seq(0, 15, by = 0.1)
lik <- rep(NA, length(theta))

for (i in 1:length(theta)){
    lik[i] <- prod(dpois(y,theta[i]))
}
```

***The prior distribution of*** θ

4.  Write a function for the prior on θ. Use a gamma distribution for the prior. The function for the prior should return a vector of gamma probability densities, one for each value of θ. It should have arguments 1) the vector for θ you created in the previous step as well as 2) the prior mean and 3) the prior standard deviation. The mean and the standard deviation, of course, will need to be moment-matched to the proper parameters of the gamma distribution.

```{r}
alpha.prior <- (mu.prior^2) / (sigma.prior^2)
beta.prior <- mu.prior / (sigma.prior^2)
scale.1 <- 1 / beta.prior
prior <- dgamma(theta, shape = alpha.prior, scale = scale.1)
```

5.  Plot the prior distribution of θ, the probability density of θ as a function of the values of θ.

```{r}
jnt <- lik * prior
plot(jnt)
```

6.  Check your moment matching by generating 100,000 random variates from a gamma distribution with parameters matched to the prior mean and standard deviation. Now compute the mean and standard deviation of the random variates. They should be very close to 10.2 and .5.

```{r}
check.joint <- rgamma(100000, shape = alpha.prior, scale = scale.1)
check.mu <- mean(check.joint)
check.sd <- sd(check.joint)
```

***The liklihood***

7.  Write a function for the likelihood. The function must use all 50 observations to compute the total likelihood across all of the data points (not the log likelihood) for each value of the vector θ. It should have arguments for the vector θ and the data. The function should create and return a vector with elements [y∣θi]. Note that this is the total probability density of all of the data for each value of θi, not the probability density of a single data point. In reality, θ is a continuous random variable, the mean of the Poisson distribution. We are discretizing it here into small intervals.

```{r}
liklihood <- function(data, para) {
  lik <- rep(NA, length(para))
  for (i in 1:length(para)){
    lik[i] <- prod(dgamma(data, para[i]))
  }
  return (lik)
}
lik2 <- liklihood(y, theta)
```

8.  Plot the likelihood of the parameter value conditional on the data, L(θi∣y) as a function of θi.

```{r}
plot(lik2 ~ theta)
```

***The joint distribution***

9.  Create a function for the joint distribution of the parameters and the data as the product of the prior and the likelihood functions. Call this function joint. The function should simply call the previous two functions and multiply them. Plot joint(theta) as a function of theta. Does this seem reasonable? Why are the values on the y axis so small? Think about what is going on here.

```{r}
joint <- function(x) {
  result <- x * lik2
  return(result)
}
plot(joint(theta) ~ theta)
jnt.theta <- joint(theta)
# seems reasonable as you they are joint liklihoods (multiplying one probability by another)
```

***The marginal probability of data***

10. Approximate the integral of the likelihood multiplied by the prior to obtain a normalization constant [y]. How would you accomplish this integration?

```{r}
sum.joint <- sum((lik * prior) * 0.1)
posterior <- ((lik * prior) / sum.joint)
```

***Putting it all together***

11. Plot the prior, a histogram of the data, the likelihood, the joint, and the posterior in a six panel layout

```{r}
par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))
plot(theta, prior, type = "l", col = "blue",
     main = "Prior", xlab = expression(theta), ylab = "")
hist(y)
plot(theta, lik, type = "l", col = "red",
     main = "Likelihood", xlab = expression(theta), ylab = "")
plot(theta, jnt, type = "l", col = "purple",
     main = "Joint", xlab = expression(theta), ylab = "")
plot(theta, posterior, type = "l", col = "black",
     main = "Posterior", xlab = expression(theta), ylab = "")
```
